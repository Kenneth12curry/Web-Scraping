#!/usr/bin/env python3
"""
Script de test pour v√©rifier les am√©liorations du scraping et des r√©sum√©s IA
"""

import requests
import json
import time
from datetime import datetime

# Configuration
BASE_URL = "http://localhost:8080"
LOGIN_URL = f"{BASE_URL}/api/auth/login"
SCRAPING_URL = f"{BASE_URL}/api/scraping/extract"

# Credentials de test
ADMIN_USERNAME = "admin"
ADMIN_PASSWORD = "admin123"

def login():
    """Connexion pour obtenir un token JWT"""
    try:
        response = requests.post(LOGIN_URL, json={
            'username': ADMIN_USERNAME,
            'password': ADMIN_PASSWORD
        })
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                return data.get('access_token')
        
        print(f"‚ùå Erreur de connexion: {response.status_code}")
        return None
    except Exception as e:
        print(f"‚ùå Erreur lors de la connexion: {e}")
        return None

def test_scraping_with_summaries(token):
    """Test du scraping avec v√©rification des r√©sum√©s IA"""
    headers = {'Authorization': f'Bearer {token}'}
    
    # Sites de test avec diff√©rents types de contenu
    test_sites = [
        {
            'url': 'https://www.lemonde.fr',
            'name': 'Le Monde',
            'expected_articles': 1
        },
        {
            'url': 'https://www.lefigaro.fr',
            'name': 'Le Figaro',
            'expected_articles': 3
        },
        {
            'url': 'https://www.liberation.fr',
            'name': 'Lib√©ration',
            'expected_articles': 2
        }
    ]
    
    print("üï∑Ô∏è Test des am√©liorations du scraping...")
    print("=" * 60)
    
    total_articles = 0
    total_with_summaries = 0
    total_processing_time = 0
    
    for i, site in enumerate(test_sites, 1):
        print(f"\nüìÑ Test {i}/3: {site['name']} ({site['url']})")
        
        try:
            start_time = time.time()
            
            response = requests.post(SCRAPING_URL, 
                json={
                    'url': site['url'], 
                    'method': 'scrapedo', 
                    'max_articles': site['expected_articles']
                },
                headers=headers
            )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                if data.get('success'):
                    articles = data.get('articles', [])
                    articles_count = len(articles)
                    articles_with_summaries = data.get('articles_with_summaries', 0)
                    method_used = data.get('method_used', 'unknown')
                    feedback = data.get('feedback', '')
                    processing_time_api = data.get('processing_time', 'N/A')
                    
                    print(f"  ‚úÖ Succ√®s")
                    print(f"    üìä Articles extraits: {articles_count}")
                    print(f"    ü§ñ Articles avec r√©sum√©s IA: {articles_with_summaries}")
                    print(f"    ‚öôÔ∏è M√©thode utilis√©e: {method_used}")
                    print(f"    ‚è±Ô∏è Temps de traitement: {processing_time_api}")
                    print(f"    üí¨ Feedback: {feedback}")
                    
                    # Analyser la qualit√© des articles
                    if articles:
                        print(f"    üìù Analyse des articles:")
                        for idx, article in enumerate(articles[:3], 1):  # Analyser les 3 premiers
                            title = article.get('title', 'Sans titre')
                            content_length = len(article.get('content', ''))
                            resume = article.get('resume', '')
                            has_resume = 'non disponible' not in resume.lower() if resume else False
                            
                            print(f"      {idx}. {title[:50]}...")
                            print(f"         Contenu: {content_length} caract√®res")
                            print(f"         R√©sum√© IA: {'‚úÖ' if has_resume else '‚ùå'} {resume[:80]}...")
                    
                    total_articles += articles_count
                    total_with_summaries += articles_with_summaries
                    total_processing_time += processing_time
                    
                else:
                    print(f"  ‚ùå √âchec: {data.get('message', 'Erreur inconnue')}")
            else:
                print(f"  ‚ùå Erreur HTTP: {response.status_code}")
                
        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
        
        # Pause entre les tests
        time.sleep(2)
    
    # R√©sum√© global
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DES AM√âLIORATIONS")
    print("=" * 60)
    
    print(f"üìà Total articles extraits: {total_articles}")
    print(f"ü§ñ Articles avec r√©sum√©s IA: {total_with_summaries}")
    print(f"üìä Taux de r√©sum√©s IA: {(total_with_summaries/total_articles*100):.1f}%" if total_articles > 0 else "N/A")
    print(f"‚è±Ô∏è Temps de traitement moyen: {(total_processing_time/3):.2f}s")
    
    # √âvaluation de la qualit√©
    print(f"\nüéØ √âvaluation de la qualit√©:")
    if total_articles > 0:
        summary_rate = (total_with_summaries/total_articles*100)
        if summary_rate >= 80:
            print(f"  ‚úÖ Excellente qualit√© des r√©sum√©s IA ({summary_rate:.1f}%)")
        elif summary_rate >= 60:
            print(f"  ‚ö†Ô∏è Bonne qualit√© des r√©sum√©s IA ({summary_rate:.1f}%)")
        else:
            print(f"  ‚ùå Qualit√© des r√©sum√©s IA √† am√©liorer ({summary_rate:.1f}%)")
    
    if total_processing_time < 30:
        print(f"  ‚úÖ Performance excellente (temps total: {total_processing_time:.1f}s)")
    elif total_processing_time < 60:
        print(f"  ‚ö†Ô∏è Performance correcte (temps total: {total_processing_time:.1f}s)")
    else:
        print(f"  ‚ùå Performance √† am√©liorer (temps total: {total_processing_time:.1f}s)")

def test_specific_improvements(token):
    """Test sp√©cifique des am√©liorations"""
    headers = {'Authorization': f'Bearer {token}'}
    
    print("\nüîß Test des am√©liorations sp√©cifiques...")
    print("=" * 60)
    
    # Test avec un site qui devrait donner de bons r√©sultats
    test_url = "https://www.lemonde.fr"
    
    try:
        print(f"üìÑ Test d√©taill√©: {test_url}")
        
        response = requests.post(SCRAPING_URL, 
            json={'url': test_url, 'method': 'scrapedo', 'max_articles': 5},
            headers=headers
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                articles = data.get('articles', [])
                
                print(f"‚úÖ Extraction r√©ussie: {len(articles)} articles")
                
                # Analyser chaque article en d√©tail
                for idx, article in enumerate(articles, 1):
                    print(f"\nüìù Article {idx}:")
                    print(f"   Titre: {article.get('title', 'N/A')}")
                    print(f"   URL: {article.get('url', 'N/A')}")
                    print(f"   Contenu: {len(article.get('content', ''))} caract√®res")
                    
                    resume = article.get('resume', '')
                    if resume and 'non disponible' not in resume.lower():
                        print(f"   R√©sum√© IA: ‚úÖ {resume}")
                    else:
                        print(f"   R√©sum√© IA: ‚ùå {resume}")
                    
                    # V√©rifier la pr√©sence d'une date
                    if article.get('date'):
                        print(f"   Date: ‚úÖ {article.get('date')}")
                    else:
                        print(f"   Date: ‚ùå Non trouv√©e")
                
                # V√©rifier les nouvelles m√©triques
                processing_time = data.get('processing_time', 'N/A')
                articles_with_summaries = data.get('articles_with_summaries', 0)
                
                print(f"\nüìä Nouvelles m√©triques:")
                print(f"   Temps de traitement: {processing_time}")
                print(f"   Articles avec r√©sum√©s: {articles_with_summaries}/{len(articles)}")
                
            else:
                print(f"‚ùå √âchec: {data.get('message', 'Erreur inconnue')}")
        else:
            print(f"‚ùå Erreur HTTP: {response.status_code}")
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")

def main():
    """Fonction principale"""
    print("üöÄ Test des am√©liorations du scraping FinData IA-M.K")
    print("=" * 60)
    
    # Connexion
    print("üîê Connexion...")
    token = login()
    if not token:
        print("‚ùå Impossible de se connecter. Arr√™t du test.")
        return
    
    print("‚úÖ Connexion r√©ussie!")
    
    # Test du scraping avec r√©sum√©s
    test_scraping_with_summaries(token)
    
    # Test sp√©cifique des am√©liorations
    test_specific_improvements(token)
    
    print("\nüéâ Test des am√©liorations termin√©!")
    print("üí° Les am√©liorations incluent:")
    print("   - R√©sum√©s IA plus fiables")
    print("   - Meilleure extraction de contenu")
    print("   - Gestion d'erreurs am√©lior√©e")
    print("   - M√©triques de performance")

if __name__ == "__main__":
    main() 