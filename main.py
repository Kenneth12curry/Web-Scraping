import os
import sys
import json
import requests
from dotenv import load_dotenv
from langchain_groq import ChatGroq
import urllib.parse

# Charger les variables d'environnement depuis .env
load_dotenv()

# Nom du modÃ¨le Groq Ã  utiliser (fintech-friendly, rapide et robuste)
MODEL_NAME = "llama3-8b-8192"

# Configuration Scrape.do
SCRAPEDO_API_KEY = os.getenv('SCRAPEDO_API_KEY')
SCRAPEDO_BASE_URL = "https://api.scraped.do"


def scrape_with_scrapedo(url: str, params: dict = {}) -> dict:
    """
    Utilise l'API Scrape.do pour rÃ©cupÃ©rer le contenu d'une page web (conforme Ã  la doc officielle).
    """
    if not SCRAPEDO_API_KEY:
        raise ValueError("Token Scrape.do manquant dans le fichier .env")

    # Construction de la query string
    query = {
        "token": SCRAPEDO_API_KEY,
        "url": urllib.parse.quote_plus(url)
    }
    query.update(params)
    full_url = "https://api.scrape.do/?" + "&".join(f"{k}={v}" for k, v in query.items())

    try:
        response = requests.get(full_url, timeout=60)
        response.raise_for_status()
        content_type = response.headers.get("Content-Type", "")
        if "application/json" in content_type:
            return response.json()
        else:
            return {"html": response.text}
    except requests.exceptions.RequestException as e:
        raise Exception(f"Erreur Scrape.do: {e}")


def get_site_content_professional(url: str) -> str:
    """
    RÃ©cupÃ¨re le contenu d'un site de maniÃ¨re professionnelle avec Scrape.do.
    """
    try:
        result = scrape_with_scrapedo(url)
        return result.get('html', '')
    except Exception as e:
        print(f"âš ï¸  Erreur Scrape.do: {e}")
        print("ğŸ”„ Fallback vers requests standard...")
        try:
            response = requests.get(url, timeout=10)
            return response.text
        except Exception as e2:
            print(f"âŒ Erreur requests: {e2}")
            return ""


def test_scrapedo_connection() -> bool:
    """
    Teste la connexion Ã  l'API Scrape.do.
    """
    if not SCRAPEDO_API_KEY:
        return False
    
    try:
        # Test simple avec une URL de test
        result = scrape_with_scrapedo("https://httpbin.org/html")
        return 'html' in result
    except Exception:
        return False


def is_dynamic_site(html: str) -> bool:
    """
    DÃ©tecte si une page est dynamique (React, Angular, Vue, etc.) pour adapter le prompt.
    """
    dynamic_keywords = [
        'window.__', 'React', 'angular', 'vue', 'data-layer', 'app-root',
        'next.js', 'nuxt', 'gatsby', 'spa', 'single-page',
        'api/', 'graphql', 'ajax', 'fetch', 'axios',
        'lazy-load', 'infinite-scroll', 'dynamic-content',
        'webpack', 'bundle', 'chunk', 'module',
        'hydration', 'vite', 'svelte', 'solidjs', 'remix', 'astro',
    ]
    script_count = html.count('<script')
    if script_count > 5:
        return True
    for kw in dynamic_keywords:
        if kw.lower() in html.lower():
            return True
    if html.count('data-') > 10:
        return True
    return False


def generate_scraping_prompt(site_url: str, is_dynamic: bool) -> str:
    """
    GÃ©nÃ¨re un prompt dÃ©taillÃ© pour obtenir un script de scraping web adaptÃ© Ã  un site donnÃ©.
    Si le site est dynamique (React, Angular, etc.), propose Selenium ou Playwright.
    Inclut maintenant l'option Scrape.do pour un travail professionnel.
    """
    if is_dynamic:
        return f"""
Tu es un expert en scraping web pour la fintech. GÃ©nÃ¨re un script Python complet qui :
- Utilise Selenium (ou Playwright si tu prÃ©fÃ¨res) pour charger dynamiquement les pages du site : {site_url}
- OU utilise l'API Scrape.do (plus professionnel) avec le token dans .env
- Pour chaque article financier, extrait le titre, l'URL, la date si disponible
- Va sur la page de chaque article pour extraire le texte principal
- Utilise urllib.parse.urljoin pour construire les URLs complÃ¨tes
- VÃ©rifie l'existence de chaque Ã©lÃ©ment avant d'y accÃ©der
- GÃ¨re la pagination (maximum 3 pages)
- Affiche les rÃ©sultats dans la console

IMPORTANT :
- Utilise des variables temporaires pour chaque extraction
- GÃ¨re les cas oÃ¹ les Ã©lÃ©ments n'existent pas
- N'inclus AUCUNE explication, seulement le code Python pur
- Le code doit Ãªtre exÃ©cutable immÃ©diatement
- Le script doit Ãªtre robuste pour les sites fintech modernes (React, Angular, etc.)

Exemple de structure robuste avec Scrape.do :
```python
import os
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
import urllib.parse

load_dotenv()
SCRAPEDO_API_KEY = os.getenv('SCRAPEDO_API_KEY')

def scrape_with_scrapedo(url):
    headers = {{'Authorization': f'Bearer {{SCRAPEDO_API_KEY}}'}}
    response = requests.post('https://api.scraped.do/scrape', 
                           headers=headers,
                           json={{'url': url, 'render': 'true', 'wait': 3000}})
    return response.json()['html']

# Utilisation
html = scrape_with_scrapedo("{site_url}")
soup = BeautifulSoup(html, 'html.parser')
# Extraction...
```

Ou avec Selenium :
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import urllib.parse
import time

options = Options()
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)
driver.get("{site_url}")
# Extraction dynamique...
```
"""
    else:
        return f"""
Tu es un expert en scraping web pour la fintech. GÃ©nÃ¨re un script Python complet qui :
- Utilise requests et BeautifulSoup pour extraire les articles du site : {site_url}
- OU utilise l'API Scrape.do (plus professionnel) avec le token dans .env
- Pour chaque article, extrait le titre, l'URL, la date si disponible
- Va sur la page de chaque article pour extraire le texte principal
- Utilise urllib.parse.urljoin pour construire les URLs complÃ¨tes
- VÃ©rifie l'existence de chaque Ã©lÃ©ment avant d'y accÃ©der
- GÃ¨re la pagination (maximum 3 pages)
- Affiche les rÃ©sultats dans la console

IMPORTANT :
- Utilise des variables temporaires pour chaque extraction
- GÃ¨re les cas oÃ¹ les Ã©lÃ©ments n'existent pas
- N'inclus AUCUNE explication, seulement le code Python pur
- Le code doit Ãªtre exÃ©cutable immÃ©diatement
- Le script doit Ãªtre robuste pour les sites fintech

Exemple de structure robuste avec Scrape.do :
```python
import os
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
import urllib.parse

load_dotenv()
SCRAPEDO_API_KEY = os.getenv('SCRAPEDO_API_KEY')

def scrape_with_scrapedo(url):
    headers = {{'Authorization': f'Bearer {{SCRAPEDO_API_KEY}}'}}
    response = requests.post('https://api.scraped.do/scrape', 
                           headers=headers,
                           json={{'url': url, 'render': 'true'}})
    return response.json()['html']

# Utilisation
html = scrape_with_scrapedo("{site_url}")
soup = BeautifulSoup(html, 'html.parser')
# Extraction...
```

Ou avec requests standard :
```python
import requests
from bs4 import BeautifulSoup
import urllib.parse

url = "{site_url}"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
# Extraction des articles...
```
"""

def query_groq(prompt: str, model: str = MODEL_NAME) -> str:
    """
    Envoie un prompt au modÃ¨le Groq via LangChain et retourne la rÃ©ponse textuelle.
    """
    try:
        llm = ChatGroq(model=model, temperature=0.0)
        response = llm.invoke(prompt)
        # Toujours retourner un str, mÃªme si le contenu est une liste ou structurÃ©
        return response.text() if hasattr(response, 'text') else str(response)
    except Exception as e:
        return f"[ERREUR] ProblÃ¨me avec l'API Groq : {e}"

def main():
    print("ğŸš€ Test de scraping avec Groq (LangChain) + Scrape.do")
    print("=" * 60)

    # VÃ©rifier la configuration
    if not SCRAPEDO_API_KEY:
        print("âš ï¸  Token Scrape.do manquant dans .env")
        print("   Le scraping utilisera requests standard")
    else:
        print("âœ… Token Scrape.do configurÃ©")
        # Tester la connexion
        print("ğŸ”„ Test de connexion Scrape.do...")
        if test_scrapedo_connection():
            print("âœ… Connexion Scrape.do rÃ©ussie")
        else:
            print("âŒ Ã‰chec de connexion Scrape.do")

    # Demander l'URL du site
    site_url = input("\nğŸŒ Entrez l'URL du site Ã  scraper : ").strip()
    if not site_url:
        site_url = "https://www.finextra.com/news"
        print(f"URL par dÃ©faut utilisÃ©e : {site_url}")

    # Choisir la mÃ©thode de scraping
    print("\nğŸ”§ Choisissez la mÃ©thode de scraping :")
    print("1. Scrape.do (recommandÃ© - professionnel)")
    print("2. Requests standard (rapide)")
    print("3. DÃ©tection automatique")
    
    choice = input("Votre choix (1-3) : ").strip()
    
    # RÃ©cupÃ©rer le contenu selon la mÃ©thode choisie
    if choice == "1" and SCRAPEDO_API_KEY:
        print("ğŸ”„ Utilisation de Scrape.do...")
        html = get_site_content_professional(site_url)
    elif choice == "2":
        print("ğŸ”„ Utilisation de requests standard...")
        try:
            html = requests.get(site_url, timeout=10).text
        except Exception as e:
            print(f"âŒ Erreur requests: {e}")
            html = ""
    else:
        print("ğŸ”„ DÃ©tection automatique...")
        if SCRAPEDO_API_KEY:
            html = get_site_content_professional(site_url)
        else:
            try:
                html = requests.get(site_url, timeout=10).text
            except Exception as e:
                print(f"âŒ Erreur requests: {e}")
                html = ""

    # DÃ©tecter si le site est dynamique
    is_dynamic = is_dynamic_site(html)
    print(f"\nğŸ“Š Site dÃ©tectÃ© comme {'dynamique' if is_dynamic else 'statique'}")

    # GÃ©nÃ©rer et envoyer le prompt
    prompt = generate_scraping_prompt(site_url, is_dynamic)
    print(f"\nğŸ”¹ Prompt envoyÃ© au modÃ¨le :\n{prompt}")

    response = query_groq(prompt)
    print(f"\nğŸ”¸ RÃ©ponse du modÃ¨le :\n{response}")

    # Option pour exÃ©cuter le code gÃ©nÃ©rÃ©
    if "[ERREUR]" not in response:
        execute = input("\nğŸ¤” Voulez-vous exÃ©cuter le code gÃ©nÃ©rÃ© ? (o/n) : ").lower().strip()
        if execute in ['o', 'oui', 'y', 'yes']:
            print("\nâš ï¸  ATTENTION : ExÃ©cution de code gÃ©nÃ©rÃ© par IA")
            print("Le code sera exÃ©cutÃ© dans un environnement sÃ©curisÃ©...")
            print("(FonctionnalitÃ© d'exÃ©cution Ã  implÃ©menter)")

if __name__ == "__main__":
    main()
